{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "\n",
    "import alignment_new\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import json\n",
    "\n",
    " \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/nikitos/HeedBook-59df779e8e5a.json'\n",
    "\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pymorphy2\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_mp3(audio):\n",
    "    audio.export('/Users/nikitos/export/current.wav', format='wav')\n",
    "    audio, rt = librosa.load('/Users/nikitos/export/current.wav')\n",
    "    display(ipd.Audio(audio, rate=rt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_to_raw(audio):\n",
    "    audio.export('/Users/nikitos/export/to_raw.wav', 'wav')\n",
    "    os.system('sox -v 0.99 /Users/nikitos/export/to_raw.wav -b 16 ./input.raw channels 1 rate 16000')\n",
    "    return './input.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_text(text):        \n",
    "        text = re.sub(r'[^\\w-]+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\s+$', '', text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('ั', 'ะต')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transcribe_file(speech_file):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    with io.open(speech_file, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code='ru-RU',\n",
    "        enable_word_time_offsets=True)\n",
    "    \n",
    "    response = client.recognize(config, audio)\n",
    "    transcript = ''\n",
    "    for result in response.results:\n",
    "        alternative = result.alternatives[0]\n",
    "        transcript = alternative.transcript\n",
    "        #print('Transcript: {}'.format(alternative.transcript))\n",
    "\n",
    "        for word_info in alternative.words:\n",
    "            curr_word = {}\n",
    "            word = word_info.word\n",
    "            curr_word['Word'] = word\n",
    "            start_time = word_info.start_time\n",
    "            end_time = word_info.end_time\n",
    "            curr_word['Begin'] = start_time.seconds + start_time.nanos * 1e-9\n",
    "            curr_word['End'] = end_time.seconds + end_time.nanos * 1e-9\n",
    "            out.append(curr_word)\n",
    "            answ = 'Word: {}, start_time: {}, end_time: {}'.format(\n",
    "                word,\n",
    "                start_time.seconds + start_time.nanos * 1e-9,\n",
    "                end_time.seconds + end_time.nanos * 1e-9)\n",
    "            \n",
    "            #print(answ)\n",
    "    return response, out, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_text_frame(transcript, ORIGINAL_TEXT, text_list):\n",
    "    phrase_list = transcript.split()\n",
    "    size_phrase = min(10, len(phrase_list))\n",
    "\n",
    "    N = size_phrase + 6\n",
    "    PERCENT = 0.8\n",
    "\n",
    "    TABOO_PHRASE = ' '.join(phrase_list[:size_phrase])\n",
    "\n",
    "    pack, length = word_search(N, PERCENT, TABOO_PHRASE, ORIGINAL_TEXT)\n",
    "    for_alignment = text_list[pack[0][0][1]:pack[0][0][1] + len(phrase_list) + 2]\n",
    "    return for_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_data_list(align1, align2, curr_list):\n",
    "    data_list = []\n",
    "    skip_index = 0\n",
    "    for i in range(len(align1)):\n",
    "        real_word = align1[i]\n",
    "        recogn_word = align2[i]\n",
    "        if ('#' in real_word) and ('#' in recogn_word):\n",
    "            skip_index += 1\n",
    "            continue\n",
    "        elif ('#' in real_word):\n",
    "            continue\n",
    "        elif ('#' in recogn_word):\n",
    "            skip_index += 1\n",
    "            continue\n",
    "        #print(real_word, recogn_word)\n",
    "        if real_word == recogn_word:\n",
    "            sample = {}\n",
    "            sample['text_word'] = real_word\n",
    "            sample['regonized_word'] = recogn_word\n",
    "            sample['start'] = curr_list[i - skip_index]['Begin']\n",
    "            sample['end'] = curr_list[i - skip_index]['End']\n",
    "            sample['confidance'] = 1\n",
    "            sample['file'] = path\n",
    "            data_list.append(sample)\n",
    "        elif real_word != recogn_word:\n",
    "            sample = {}\n",
    "            sample['text_word'] = real_word\n",
    "            sample['regonized_word'] = recogn_word\n",
    "            sample['start'] = curr_list[i - skip_index]['Begin']\n",
    "            sample['end'] = curr_list[i - skip_index]['End']\n",
    "            sample['confidance'] = 0\n",
    "            sample['file'] = path\n",
    "            data_list.append(sample)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_search(N, PERCENT, TABOO_PHRASE, ORIGINAL_TEXT):\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    sns.set() \n",
    "    \n",
    "    def normalize_word(word):\n",
    "        # todo: hardcode\n",
    "        for i in range(10):\n",
    "            prev_word = word\n",
    "            word = morph.parse(word)[0].normal_form\n",
    "            if word == prev_word:\n",
    "                return word\n",
    "        return word\n",
    "\n",
    "    # text goes to normal form\n",
    "    def normalize_text(text):\n",
    "        return ' '.join([normalize_word(word) for word in text.split(' ')])\n",
    "\n",
    "    # generation list of synonyms\n",
    "    def find_taboo(words, taboo_synonyms_df):\n",
    "#         words: taboo words to find\n",
    "#         taboo_synonyms_df: found synonyms in text. pd.DataFrame(columns=['word']) with index, corresponding to list of <words> from the text\n",
    "\n",
    "#         return: a list of values of found words: [[<word>, <index>, <synonym or original word found in text>], ...]\n",
    "#         example: [[['a', 0, 'a']], [['b', 2, 'b']], [['c', 3, 'd']]]\n",
    "        words = list(words)\n",
    "\n",
    "        res_values = []\n",
    "        for i in range(len(words)):\n",
    "            print(i, taboo_synonyms_df, words)\n",
    "            res_values.append([words[i]] + taboo_synonyms_df.iloc[i].tolist())\n",
    "        return res_values\n",
    "    \n",
    "    \n",
    "    text = normalize_text(ORIGINAL_TEXT)\n",
    "    words = text.split(' ')\n",
    "    words_df = pd.DataFrame(words, columns=['word']).reset_index()\n",
    "    taboo_phrase = normalize_text(TABOO_PHRASE)\n",
    "    taboo_words = taboo_phrase.split(' ')\n",
    "\n",
    "\n",
    "    taboo_synonyms = taboo_words\n",
    "        \n",
    "    taboo_index = [i for i, word in enumerate(words) if word in taboo_synonyms]\n",
    "    taboo_words_df = words_df.iloc[taboo_index, :]\n",
    "   \n",
    "    cond1 = taboo_words_df['index'] - taboo_words_df['index'].shift(1).fillna(0) < N\n",
    "    cond2 = taboo_words_df['index'].shift(-1).fillna(0) - taboo_words_df['index'] < N\n",
    "    taboo_words_df = taboo_words_df[cond1 | cond2]\n",
    "    res = []\n",
    "    for i in range(taboo_words_df.shape[0]):\n",
    "        index_low = taboo_words_df.index[i]\n",
    "        index_high = index_low + N\n",
    "        indices_in = [i for i in taboo_words_df.index if i > index_low - 1 and i < index_high + 1]\n",
    "        taboo_words_in = taboo_words_df.loc[indices_in]\n",
    "        pack = find_taboo(taboo_words, taboo_words_in)\n",
    "        pack = list(sorted(pack, key=lambda x: x[1]))\n",
    "        if len(pack) > round(PERCENT * len(taboo_words)) - 1:\n",
    "            res.append(pack)\n",
    "            break\n",
    "\n",
    "    \n",
    "    final_pack = []\n",
    "\n",
    "    original_words = ORIGINAL_TEXT.split(' ')\n",
    "    beg_last = 0\n",
    "    end_last = 0\n",
    "    j = 0\n",
    "    for pack in res:\n",
    "        pack_synonyms = list(set([x[2] for x in pack]))\n",
    "        beg, end = pack[0][1], pack[-1][1] + 1\n",
    "        real_end = beg + N\n",
    "        if end - beg > N:\n",
    "            beg_last = beg\n",
    "            end_last = end\n",
    "            continue\n",
    "\n",
    "        if end == end_last:\n",
    "            beg_last = beg\n",
    "            continue\n",
    "\n",
    "        tmp = [0] * len(taboo_words)\n",
    "        for i in range(beg, real_end):\n",
    "            if words[i] in taboo_words:\n",
    "                index = taboo_words.index(words[i])\n",
    "                tmp[index] = 0 \n",
    "\n",
    "        s = 0\n",
    "        for i in range(len(tmp)):\n",
    "             s = s + tmp[i]\n",
    "\n",
    "        if s!= 0:\n",
    "            continue\n",
    "\n",
    "        final_pack.append(pack)\n",
    "        break\n",
    "        j = j + 1\n",
    "\n",
    "        end_last = end\n",
    "        beg_last = beg\n",
    "        \n",
    "    return final_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_from_audiobooks(audio_path, text_path):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/nikitos/HeedBook-59df779e8e5a.json'\n",
    "    client = speech.SpeechClient()\n",
    "    audio = AudioSegment.from_mp3(audio_path)\n",
    "    \n",
    "  \n",
    "    ORIGINAL_TEXT = open(text_path, encoding='windows-1251').read()\n",
    "    ORIGINAL_TEXT = clear_text(ORIGINAL_TEXT)\n",
    "    \n",
    "    text_list = ORIGINAL_TEXT.split()\n",
    "    \n",
    "    total_mls = len(audio)\n",
    "    step = 30000\n",
    "    number = 1\n",
    "    data = []\n",
    "    while step * number < total_mls:\n",
    "        start_fr = step * number\n",
    "        number += 1\n",
    "        end_fr = step * number\n",
    "        x, curr_list, transcript = transcribe_file(export_to_raw(audio[start_fr:end_fr]))\n",
    "        transcript = clear_text(transcript)\n",
    "        phrase_list = transcript.split()\n",
    "        for_alignment = search_text_frame(transcript, ORIGINAL_TEXT, text_list)\n",
    "        \n",
    "        align1, align2 = alignment_new.needle(for_alignment, phrase_list)\n",
    "        \n",
    "        list_for_frame = making_data_list(align1, align2, curr_list)\n",
    "        data += list_for_frame\n",
    "        \n",
    "    start_fr = step * number\n",
    "    end_fr = total_mls - 1\n",
    "    x, curr_list, transcript = transcribe_file(export_to_raw(audio[start_fr:end_fr]))\n",
    "    transcript = clear_text(transcript)\n",
    "    phrase_list = transcript.split()\n",
    "    for_alignment = search_text_frame(transcript, ORIGINAL_TEXT, text_list)\n",
    "        \n",
    "    align1, align2 = alignment_new.needle(for_alignment, phrase_list)\n",
    "        \n",
    "    list_for_frame = making_data_list(align1, align2, curr_list)\n",
    "    data += list_for_frame\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     index word\n",
      "18     18    ะฒ\n",
      "24     24   ะฝะฐ\n",
      "28     28    ะฒ ['ััะฐะฒะฝะพะฒะตัะธัั', 'ัะฒะตัะปัะน', 'ะฝะฐัะฐัั', 'ััะผะฝัะน', 'ะฝะตัะผะพััั', 'ะฝะฐ', 'ััะพั', 'ะฑะปะตััััะธะน', 'ัะฐะทัะฐะฑะพัะบะฐ', 'ะฒ']\n",
      "1     index word\n",
      "18     18    ะฒ\n",
      "24     24   ะฝะฐ\n",
      "28     28    ะฒ ['ััะฐะฒะฝะพะฒะตัะธัั', 'ัะฒะตัะปัะน', 'ะฝะฐัะฐัั', 'ััะผะฝัะน', 'ะฝะตัะผะพััั', 'ะฝะฐ', 'ััะพั', 'ะฑะปะตััััะธะน', 'ัะฐะทัะฐะฑะพัะบะฐ', 'ะฒ']\n",
      "2     index word\n",
      "18     18    ะฒ\n",
      "24     24   ะฝะฐ\n",
      "28     28    ะฒ ['ััะฐะฒะฝะพะฒะตัะธัั', 'ัะฒะตัะปัะน', 'ะฝะฐัะฐัั', 'ััะผะฝัะน', 'ะฝะตัะผะพััั', 'ะฝะฐ', 'ััะพั', 'ะฑะปะตััััะธะน', 'ัะฐะทัะฐะฑะพัะบะฐ', 'ะฒ']\n",
      "3     index word\n",
      "18     18    ะฒ\n",
      "24     24   ะฝะฐ\n",
      "28     28    ะฒ ['ััะฐะฒะฝะพะฒะตัะธัั', 'ัะฒะตัะปัะน', 'ะฝะฐัะฐัั', 'ััะผะฝัะน', 'ะฝะตัะผะพััั', 'ะฝะฐ', 'ััะพั', 'ะฑะปะตััััะธะน', 'ัะฐะทัะฐะฑะพัะบะฐ', 'ะฒ']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-a513b6f9835d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_from_audiobooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./generation_p/book/0002.mp3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./generation_pi.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-340f7037e871>\u001b[0m in \u001b[0;36mdata_from_audiobooks\u001b[0;34m(audio_path, text_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclear_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mphrase_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mfor_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_text_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORIGINAL_TEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0malign1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignment_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneedle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_alignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-bfc3c461c855>\u001b[0m in \u001b[0;36msearch_text_frame\u001b[0;34m(transcript, ORIGINAL_TEXT, text_list)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTABOO_PHRASE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize_phrase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTABOO_PHRASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORIGINAL_TEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfor_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfor_alignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-cdc0f5996bb3>\u001b[0m in \u001b[0;36mword_search\u001b[0;34m(N, PERCENT, TABOO_PHRASE, ORIGINAL_TEXT)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mindices_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaboo_words_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mindex_low\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindex_high\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtaboo_words_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaboo_words_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mpack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_taboo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaboo_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaboo_words_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mpack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPERCENT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaboo_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-cdc0f5996bb3>\u001b[0m in \u001b[0;36mfind_taboo\u001b[0;34m(words, taboo_synonyms_df)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaboo_synonyms_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mres_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtaboo_synonyms_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "data = data_from_audiobooks('./generation_p/book/0002.mp3', './generation_pi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
